====================================
Prototypes assessment and comparison 
====================================

This document has two objectives:

* Assess which of the two prototypes are suitable as a model for
  the production library, if any.
* Compare both prototypes in terms of performance, programming
  model, ease of maintenance, user friendliness (programming using
  the library, deploying applications)...

Prototypes characteristics
==========================

Prototype 1 - Terracotta-based
------------------------------
Terracotta is a technology that provides network-attached memory.
It provides a virtual shared Java heap that any machine on the 
network may use transparently. It has no API (instead, it instruments
java bytecode to propagate changes to shared objects across the
cluster). Object identity is maintained automatically.

The idea of our first prototype implementation is to declare
a VirtualSpace as a shared root and let "slave" applications
(one slave for each screen or machine) scan repeatedly the scene 
graph (ie the VirtualSpace) to draw a part of the scene. 

Salient traits of the first prototype:

* No/minimal modifications on the trunk (add rather than modify)
* Minimal modifications to distribute an application
* Minimal API changes (No API changes for glyphs except images
  which need passing metadata rather than data around - ie URLs
  or filenames).
* *Pull model* (slaves refresh scene state from master), optimised
  through Terracotta use (only changed parts of the scene are transmitted).
* Adding a new Glyph subtype is easy (configuration + optional
  Beanshell script in tc-config.xml)
* *Code-by-exception* approach: Glyph objects (and others) will
  usually be properly distributed, changes are needed sometimes
  and are performed by transient declarations and Beanshell snippets
* Networking is unicast (implementation detail, but cannot be
  changed)
* Needs to work together with the zvtm threading model
  (rendering threads contending for glyph access)
* Requires distributed garbage collection (provided by Terracotta,
  but assumes a well-behaved application)

Prototype 2 - JGroups and AspectJ based
---------------------------------------
The second prototype is based around the idea of "manually"
replicating the state of a VirtualSpace across every node
of the cluster (i.e. mirror the master VirtualSpace in each slave 
application). We use our own serialization device and network 
transport, and we (library implementers) are responsible for
maintaining object identity (replacing pointers with integers).

Salient traits of the second prototype:

* No/minimal modifications on the trunk (add rather than modify -- this
  could allow providing a separate clustering library, thus avoiding 
  single-host users of ZVTM any extra dependency or penalty)
* *Push model* (master sends changes to slaves)
* Minimal API changes (No API changes for glyphs except images
  which need passing metadata rather than data around - ie URLs
  or filenames).
* JGroups is the networking technology, AspectJ enables us
  to make API changes mostly transparent and (hopefully) keep
  clustering dependencies independant of the rest (ie we want to
  build a clustered or non-clustered version of ZVTM without having
  to pull clustered dependencies and with no/minimal performance impact
  in the non-clustered case).
* Adding a new Glyph subtype is more tedious than with prototype 1
* *Code-all* approach: All interesting state needs to be passed explicitely
  to the slaves by creating the appropriate network messages (from the
  library dev point-of-view -- by design it is transparent from the library
  user or application developer point of view) 
* Networking is (usually) multicast, can be wrapped into unicast
  channels when necessary (e.g. machines distributed on different 
  networks...) by configuring a text file. There is typically
  only one such file. 
* Extensive network tuning capabilities
  (see JGroups protocol stack). Framework for building state transfer
  protocols. Built-in tunneling capabilities etc.
* Uses message passing: less affected by zvtm threading model
  (no distributed shared memory)
* Plays nice with scripting engines, as long as the clustering library
  is "self-contained" with regards to AspectJ, i.e. client programs do not
  need weaving (the prototype was modified to gain this property). 
  Moreover, client compilation and deployment is made simpler if 
  the client application programmer does not need to care about 
  AspectJ weaving. Tested engines include Jython and Beanshell. 

Decision Criteria
=================

* Performance

  - Static scene startup
  - Dynamic scene startup
  - Dynamic scene refresh rate
  - Vary the number of hosts, total glyphs, animated glyphs.

* Ease of use / extensibility / implementation

  - Ease of deployment
  - Number and "weight" of dependencies: Terracotta has numerous 
    dependencies, including a special server process
    and database. The second prototype has lighter dependencies (no
    unneeded persistence/failover capabilities, no dedicated shared heap
    process that requires a database). This also means 
    less memory consumption.
  - Maven integration: Terracotta has good Maven integration.
    AspectJ has a few problems(outdated Maven plugin?).
    JGroups has good Maven integration
  - Ease of implementation
    The Terracotta prototype was easier / less tedious to implement
    but avoiding thread contention requires efforts (possibly important)
  - Ease of extensibility for customers (application writers)
    Terracotta wins here, as new Glyph-derived classes need some scaffolding
    to be clustered using the second prototype. 

* Other

  - Licencing issues: advertising clause in Terracotta which 
    may need attention.

Performance
===========

Dynamic scene, one host
-----------------------
Average number of position updates per glyph per second for
different numbers of animated glyphs in the scene. Higher is better.
Our target, given WILD dimensions, is to run 300 concurrent animations
smoothly.

* Prototype 1

  - Method of measurement: update counter in AnimationManager (warning: 
    differs from 2nd prototype measurement technique, but consistent
    with observed behavior)
  - 200 concurrent anims: 4.37 updates/glyph/sec (not smooth) 
  - 500 concurrent anims: 1.6 updates/glyph/sec (not smooth) 
  - 2000 concurrent anims: ?

* Prototype 2

  - Method of measurement: received message counter
    in SlaveUpdater
  - 200 concurrent anims : 38.5 updates/glyph/sec (7700 msg/sec - smooth)
  - 500 concurrent anims : 24 updates/glyph/sec (12000 msg/sec - limit
    for smooth animation?)
  - 2000 concurrent anims: 7 updates/glyph/sec (14000 msg/sec - not smooth)

Static scene (camera movement only)
-----------------------------------
*Startup* time for different number of glyphs, one host.
We do not have target numbers for this, we just want to be reasonably
quick.

* Prototype 1

  - Method of measurement: stopwatch. 
    Measurements are approximate. 
  - 1000 glyphs  : 35 or 47 sec (master only, slave+master)
  - 10000 glyphs : OOME (first try - which process? DSO server? is that 
    due to the n+1 dynamic arrays in ZVTM? -- how do they play with DGC?) 
  - 100000 glyphs: ?

* Prototype 2

  - Method of measurement: stopwatch. 
    Measurements are approximate. Launch slave, wait until ready
    and launch master. Only master launch time is reported.
  - 1000 glyphs  : <2s
  - 10000 glyphs : <2.5s
  - 100000 glyphs: approx. 45s (non-clustered version: 16s)

Dynamic scene, multiple hosts
-----------------------------

* Prototype 2 (no comparison w/ prototype 1, just assessing
  suitability)

  - Method of measurement: received message counter in SlaveUpdater
  - 200 concurrent anims :
  - 500 concurrent anims :
  - 2000 concurrent anims:

A first attempt at a conclusion
===============================

* Terracotta looked promising but is probably not adapted to
  this use case (a large number of slaves scanning a large
  scene graph continuously)

* Message passing is easier than extending the threading model
  and should yield better performance

* Depending on Terracotta is somewhat "heavier" than depending on 
  JGroups + AspectJ 

* Distributed garbage collection may require careful tuning
  (although in the OOME problem I suspect that ZVTM n+1 dynarrays are the
  cause). Using a "share-nothing" approach (ie prototype 2) lets 
  us avoid this issue.

* The second prototype has met our initial performance targets.
  There are also no major development/deployment/use hurdles, so
  *modeling the final library after the second prototype looks like
  our best option so far*. 

