\newpage
%%%%%%%%%%%%%%%%%%%%
\section{Basic Concepts}
\label{sec:basic}

%%%%%%%%%%
\subsection{Overview of concepts}
\label{sec:concepts}

The toolkit is based on the metaphor of infinite\footnote{The virtual space coordinate system uses double-precision floating-point values.} universes called {\em virtual spaces} that can be observed through movable and zoomable {\em cameras} and contain potentially large amounts of graphical objects called {\em glyphs}: geometrical shapes, bitmap images, text.

\begin{figure}
\begin{center}
       \includegraphics[width=4cm,angle=-90]{images/vtmVS.pdf} 
       \hspace{2cm}
       \includegraphics[width=4cm,angle=-90]{images/vtmLayers.pdf}
\end{center}
       \caption{Schematic representation of virtual spaces, cameras, glyphs, views and layers}
   \label{fig:vtmVS}
\end{figure}

All glyphs rely on the same polymorphic object model. A glyph belongs to a specific virtual space, but can be observed through different cameras simultaneously as each virtual space can contain multiple cameras as shown in Figure \ref{fig:vtmVS}-a. Cameras are associated with viewports called {\em views} which correspond to windows in the user interface. As we will see in section \ref{sec:lenses}, various types of magnification lenses can be activated in views. If more than one camera is associated with a given view, each camera paints its content on a transparent layer (Figure \ref{fig:vtmVS}-b). The glyph graphical model features alpha channel support; glyphs can therefore be opaque, translucent or transparent.

Translucency is one of several orthogonal visual variables that define a glyph. Modifications to these variables can be animated using various temporal schemes (see section \ref{sec:animations}). Camera translations and altitude changes can also be animated, as well as magnification lens modifications, position and size of portals (views within views), etc.

Input event handling is managed through high-level callbacks associated with each view. Each callback method provides context about the event such as the list of objects intersected by the cursor.

On top of these concepts are implemented various interaction techniques that can easily be combined: zoomable user interfaces, superimposed translucent layers, fisheye lenses, rate-based scrolling, speed-dependent automatic zooming and more.

%%%%%%%%%%
\subsection{Hiding low-level operations}
\label{sec:lowlevel}

Toolkits such as Java2D are powerful but difficult to use, requiring the programmer to deal with low-level graphical operations and implementation problems. ZVTM allows the programmer to consider the task at a more abstract level by automatically handling the following operations:

{\bf Graphical Object Model: }ZVTM is a structured graphics toolkit. The developer manipulates actual graphical objects whose properties can be changed, not low-level primitives as found in Java2D's Graphics2D component. Operations can be performed on glyphs, either abruptly or through animations, and information about their status can easily be accessed.

{\bf Clipping:} the toolkit is designed to handle many glyphs while maintaining an acceptable refresh rate. Two ZUI\footnote{Zoomable User Interface}-aware clipping algorithms contribute to this: an analytical clipping pass that determines whether each glyph should be projected and painted or not (based on its bounding box in virtual space), and an optional top-down pass that can detect some occlusion configurations and ignore glyphs that will not be visible in the final rendering (taking glyph translucency into account).

{\bf Multi-threading and consistency:} multiple cameras associated with different views can exist simultaneously, observing the same or different regions of (possibly different) virtual space(s). A glyph observed through different cameras simultaneously is actually a single object with multiple projected coordinates. As a consequence, the synchronization and consistency of multiple views is automatic.

{\bf Repaint requests:} views are refreshed lazily, i.e., only upon request. Such repaint requests are fired automatically, so that the programmer only has to assign new values to visual properties. Each camera is associated with a view that runs in its own thread. Each thread tries to provide a frame rate as close to 25 images per second as possible (unless specified otherwise).

{\bf Animation management:} all camera, glyph, portal and lens animations are handled by the same module, which offers a simple and unified API based on Sun's Timing Framework\fref{https://timingframework.dev.java.net/} for their declarative specification. The module manages a queue of pending animations and can handle concurrent animations affecting orthogonal visual variables of the same object.

{\bf Hardware acceleration:} various graphics acceleration methods are available, such as Java Volatile Images and the OpenGL rendering pipeline. ZVTM features different types of views that provide hardware acceleration based on these methods while sharing the exact same API.

%%%%%%%%%%
\subsection{Virtual Space Manager}

The virtual space manager \cl{fr.inria.zvtm.engine.VirtualSpaceManager} is the main entry point in ZVTM. It is the first object instantiated by the client application. There is only one instance of it.

\codebox{VirtualSpaceManager vsm = VirtualSpaceManager.INSTANCE;}

Many operations, such as creating virtual spaces and views will take place through the virtual space manager (which we abbreviate VSM from now on).

If ZVTM is instantiated within an applet, you will probably need to call the following method when instantiating the JApplet:

\codebox{getRootPane().putClientProperty("defeatSystemEventQueueCheck", Boolean.TRUE);}

%%%%%%%%%%
\subsection{Virtual Spaces}

Graphical objects are put on infinite\footnote{Actually bounded by the representation of 64-bit integers in the current Java implementation, i.e., $\pm 9 \times 10^{18}$.} 2D canvas (or surfaces), called {\em virtual spaces} \cl{fr.inria.zvtm.engine.VirtualSpace}, in which they have a specified position \cd{vx,vy}. Virtual spaces use a Cartesian coordinate system, meaning that \cd{vx} and \cd{vy} can be negative. As opposed to most computer screen coordinate systems, the Y axis is oriented upward.

There can be an arbitrary number of virtual spaces. This provides an easy logical separation between graphical scenes/repre\-sen\-tations that are independent from one another.

The following creates a virtual space whose name is \cd{foo}.

\codebox{VirtualSpace vs = vsm.addVirtualSpace("foo");}

Glyphs are put on the surface of virtual spaces. A given glyph can only belong to one virtual space.

\begin{SaveVerbatim}{CodeVerb}
VCircle c = new VCircle(0, 0, 0, 10, Color.WHITE);
vs.addGlyph(c);
\end{SaveVerbatim}
\fbox{\BUseVerbatim[boxwidth=0.99\columnwidth]{CodeVerb}}

%%%%%%%%%%
\subsection{Cameras, Views and Layers}

%%%%%
\subsubsection{Cameras}

Even though we present them as 2D {\em surfaces} on which we put graphical objects, virtual spaces are logical constructs. They are not directly presented on screen. Instead, cameras \cl{fr.inria.zvtm.engine.Camera} are associated with virtual spaces. A camera observes a virtual space, with the ``line of sight'' always perpendicular to the virtual space's surface. A camera has a position \cd{x,y} in the virtual space coordinate system, as well as an altitude \cd{alt}. See Figure \ref{fig:vtmVS}. Of course, each camera's position and altitude can be changed at any time, whether abruptly or through an animation (see Section \ref{sec:animations}).

As for glyphs, cameras are added to virtual spaces:

\codebox{Camera cam = vs.addCamera();}

An arbitrary number of cameras can be added to each virtual space. One camera, however, can only belong to one virtual space.

%%%%%
\subsubsection{Views}

Associating a camera with a virtual space is however not sufficient for creating a visual representation on screen. Cameras do not display anything by themselves. Rather, they define what region of the virtual space is observed through them when associated with a {\em view}. A view is an actual window displayed on screen to the user through the windowing system. It has a width and height. Together with the camera's position and altitude, these parameters define the rectangular region of the virtual space observed through the camera. See Figure \ref{fig:vtmVS}.

There are two main types of views \cl{fr.inria.zvtm.engine.View}: external views \cl{EView, GLEView}, which are managed by the windowing system, e.g., Mac OS X, Windows, X11/Xorg; panel views \cl{PView}\footnote{Formerly miscalled AppletView because it was used mostly within a Swing JApplet.}, which can be inserted in any Swing hierarchy.

Views with the \cd{GL} prefix are OpenGL views (they use Java2D's OpenGL rendering pipeline). They require that pipeline to be enabled at runtime using the following flag:

\codebox{-Dsun.java2d.opengl=true}

Other views (without a prefix) use standard \cd{BufferedImage}s to draw their content offscreen and the copy it to the display. The latter are the only views that support magnification lenses (Section \ref{sec:lenses}).

A view is instantiated through the VSM:

\begin{small}
\begin{SaveVerbatim}{CodeVerb}
Vector cameras = new Vector();
cameras.add(cam);
View v = vsm.addFrameView(cameras, "fooview", View.STD_VIEW, 800, 600, true, true);
\end{SaveVerbatim}
\fbox{\BUseVerbatim[boxwidth=0.99\columnwidth]{CodeVerb}}
\end{small}

There are a number of variations depending on whether you want to attach a menu bar and/or status bar to the view, make it decorated with a title bar or not, and make it visible or not at instantiation time.

Special case of panel views:

\codebox{JPanel p = vsm.addPanelView(cameras, "fooView", 800, 600);}

This method returns the JPanel encapsulated by the view instead of returning the view itself, for straightforward (possible inlined) integration within a Swing component hierarchy. The \cd{View} instance itself can easily be retrieved by a call to:

\codebox{View v = vsm.getView("fooview");}

An external view can be set to run in full-screen exclusive mode, as is possible with any JFrame. Usually the View will have been initialized undecorated (no title bar).

\begin{small}
\begin{SaveVerbatim}{CodeVerb}
import java.awt.GraphicsEnvironment;
import java.awt.GraphicsDevice;

[...]

GraphicsDevice gd =
    GraphicsEnvironment.getLocalGraphicsEnvironment().getDefaultScreenDevice();
gd.setFullScreenWindow((JFrame)v.getFrame());}
\end{SaveVerbatim}
\fbox{\BUseVerbatim[boxwidth=0.99\columnwidth]{CodeVerb}}
\end{small}

%%%%%
\subsubsection{Layers}

You may have noticed in the previous section that we are not passing the camera directly to the view construction method. Instead, we are putting it in a \cd{Vector}. The reason is that a view can be made of several cameras. Each camera is associated with a {\em layer} within the view.Each layer is independent from the others. Each camera can be moved independently from the others (they can also be sticked to one another). Layers (and thus cameras) can be switched on and off. Each layer is by default transparent. Graphical objects seen through the different cameras associated with each layer simply get painted on top of each others. As we will see in Section \ref{sec:glyphs}, glyphs can be translucent or even transparent, in which case glyphs on lower-level layers will show through them.

\begin{small}
\begin{SaveVerbatim}{CodeVerb}
Vector cameras = new Vector();
cameras.add(foocam);
cameras.add(barcam);
cameras.add(lastcam);
View v = vsm.addFrameView(cameras, "3_layer_view", View.STD_VIEW, 800, 600, true, true);
\end{SaveVerbatim}
\fbox{\BUseVerbatim[boxwidth=0.99\columnwidth]{CodeVerb}}
\end{small}

Cameras belonging to different virtual spaces can be combined in the same view. A given camera can only be associated with one layer in one view (this restriction is actually not a hard one, we have however never tested the case where a camera was associated with more than one layer, so there might be unknown issues).

Only one layer is active (has focus) at a given time. The following example gives focus to the 3rd camera in view \cd{v}:

\codebox{v.setActiveLayer(2);}

This makes the associated camera active (provided the view is the active one, i.e., the one which has input focus).

%%%%%
\subsubsection{Drawing stack, Glyph z-index, and Java2D painters}

Glyphs in a view get drawn as follows:
\begin{itemize}
\item layers get drawn one after another, starting with the one of lowest index ;
\item for a given layer, all glyphs seen through the camera (that belong to the associated virtual space) get drawn according to their position in that virtual space's drawing stack. Glyphs get drawn from the bottom of the stack (lowest index) to the top of the stack (highest index).
\end{itemize}

When adding a new glyph to a virtual space, it gets inserted at the top of the drawing stack. There are various methods that let you manage the position of glyphs in a drawing stack. \cd{VirtualSpace} has methods \cd{atBottom}, \cd{onTop}, \cd{above} and \cd{below} that let you change the position of a given glyph within the drawing stack.

Position in the drawing stack can also be changed through the glyph's z-index. This is very convenient when managing groups of objects. A glyph's z-index is specified at instantiation time as a constructor parameter. Calling methods \cd{atBottom}, \cd{onTop}, \cd{above} and \cd{below} might affect a glyph's z-index.

\begin{figure}[!ht]
\centering
  \fbox{\includegraphics[width=10cm]{images/j2dp.png}}
  \caption{Example of use of a Java2DPainter (logo in the bottom-right corner, memory gauge in the bottom-left corner)}
  \label{fig:j2dp}
\end{figure}

ZVTM also provides developers with convenient callback methods for calling raw Java2D instructions at various stages of the rendering process, through the \cl{fr.inria.zvtm.engine.Java2DPainter} interface. This can be useful, e.g., for drawing a static label in a corner of the view, a logo or a memory  gauge (see Figure \ref{fig:j2dp}).

Java2DPainter callbacks can be set at the following steps of the rendering process:
\begin{itemize}
\item \cd{BACKGROUND}: method gets called after background has been painted and before any glyph gets painted;
\item \cd{FOREGROUND}: method gets called after all glyphs on all layers have been painted, but before any magnification lens is applied;
\item \cd{AFTER\_LENSES}: method gets called after the magnified content of lenses has been painted;
\item \cd{AFTER\_PORTALS}: method gets called after portals have been drawn.
\end{itemize}

\begin{small}
\begin{SaveVerbatim}{CodeVerb}
class D2P implements Java2DPainter {

  public void paint(Graphics2D g2d, int viewWidth, int viewHeight){
    g2d.drawString("aString", 10, viewHeight-20);
  }

}

...
  v.setJava2DPainter(new D2P(), Java2DPainter.FOREGROUND);
...

\end{SaveVerbatim}
\fbox{\BUseVerbatim[boxwidth=0.99\columnwidth]{CodeVerb}}
\end{small}

%%%%%%%%%%
\subsection{Portals}

A portal is a subregion of a view that displays something different. Typically, a portal will display what is seen through another camera than the camera associated with the main view\cl{fr.inria.zvtm.engine.CameraPortal}. Camera portals can be considered as smaller views inset within views. Figure \ref{fig:portal} shows an example of a camera portal displaying an overview of a virtual space containing a map. One camera (associated with the main view) observes Central America. This camera is moved freely by the user (position and altitude). A second fixed camera is positioned so as to observe the entire world (associated with the portal in the lower-left corner). See Section \ref{sec:bifocal} for more information about bi-focal displays.

\begin{small}
\begin{SaveVerbatim}{CodeVerb}
Camera detailCam = ...;
Camera overviewCam = ...;
Vector cams = new Vector();
cams.add(detailCam);
View v = vsm.addFrameView(cams, ...);
Portal p = new CameraPortal(0, 0, 100, 50, overviewCam);
vsm.addPortal(p, v);
\end{SaveVerbatim}
\fbox{\BUseVerbatim[boxwidth=0.99\columnwidth]{CodeVerb}}
\end{small}

\begin{figure}[!ht]
\centering
  \fbox{\includegraphics[width=16cm]{images/portal.png}}
  \caption{An overview implemented using a portal (lower-right inset)}
  \label{fig:portal}
\end{figure}

%%%%%%%%%%
\subsection{Input events}
\label{sec:input}

It is possible to associated the same event handler with all layers in a view, or to associate different event handlers with each layer of the view. The input event handling interface works in a way similar to conventional AWT/Swing event listeners (using callbacks). There is a single interface for all ZVTM events: \cl{fr.inria.zvtm.engine.ViewEventHandler}. A convenience (default) implementation, which does nothing, is available for subclassing: \cl{DefaultEventHandler}.

\cd{ViewEventHandler} features callbacks for conventional pointing device events (move, drag, button press/release/click, mouse wheel) and keyboard events. Additional events are triggered when the cursor enters or exits a glyph, unless this glyph has been declared as not sensitive \cl{Glyph.setSensitivity(boolean b)}.

Portals have their own event interface \cl{fr.inria.zvtm.engine.PortalEventHandler}, that is used to listen for cursor entry/exit.

%%%%%%%%%%
\subsection{Graphical Object Model}
\label{sec:glyphs}

As stated in \cite{Green96}, ``{\em data must be presented in a usable form before it becomes information, and the choice of representation affects the usability}''. The representation system, and thus the graphical object model, play a central role in converting data into information. Low-level graphical APIs provide large sets of powerful drawing primitives that address many programmers' and designers' needs. However, these primitives are often specific to the associated geometrical shape, and APIs suffer from the lack of a unified set of instructions for manipulating heterogeneous graphical objects. Moreover, these instructions often rely on machine-oriented models for encoding geometrical shapes. Such models have advantages (e.g. performance) but do not make the mapping of data to visual variables straightforward.

ZVTM's graphical object model is inspired by Bertin's perceptual dimensions \cite{bertin83} and the Visual Abstract Machine's visual type system \cite{Vion-Dury97}. The model uses encapsulation to provide the programmer with a polymorphic instruction set for manipulating all graphical objects (glyphs), no matter their actual shape and appearance.

All glyphs are defined by the following orthogonal attributes: the cartesian coordinates of the shape's centroid, the size of the shape's bounding circle, the shape's orientation, its border and fill colors, associated with an optional alpha channel for translucency. Basic predefined shapes are fully defined by these attributes. Other, more complex, shapes may require additional attributes. For instance, {\em VShape} glyphs support an arbitrary number of vertices, whose position within the bounding circle is represented by a normalized float (see \cite{Vion-Dury97} for more details); {\em VImage} requires a pointer to an actual bitmap resource; rectangular shapes require a width and height, or use the above-mentioned size in combination with an aspect ratio; curved paths ({\em DPath}, made of segments, quadratic curves and cubic curves) require a series of control points; etc. Glyphs can also be composed of other glyphs and still define polymorphic operations (resizing, reorienting, translating, coloring).

\begin{figure}[!ht]
\centering
   \includegraphics[width=8cm]{images/glyphFactory.pdf}
   \caption{Direct manipulation interface for the instantiation of glyphs.}
   \label{fig:vshape}
\end{figure}

The resulting representation system, with its orthogonal visual variables that mirror perceptual dimensions, makes mapping data to graphical attributes easy. A direct manipulation user interface (Figure \ref{fig:vshape}) facilitates the dynamic definition of glyphs, drawing a parallel with \cite{douglas99} which demonstrates the importance, for color selection, of a well-designed interface over the supposed intuitiveness of color models.

